{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "990194a9",
      "metadata": {},
      "source": [
        "# Phoneme seq2seq with CMUdict (encoder-decoder)\n",
        "Train a Transformer encoder-decoder with cross-entropy to map noisy phoneme sequences back to clean phoneme sequences using LibriSpeech text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "96e6a0ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e7fd51",
      "metadata": {},
      "source": [
        "# Rebuild LibriSpeech sentences\n",
        "Use LibriSpeech text in notebooks/data to regenerate librispeech_sentences.pkl into notebooks/data and data/ before loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "21902fb9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning 1443 text files under c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\LibriSpeech\\books\\ascii ...\n",
            "Collected 6,078,025 sentences after filtering + deduplication.\n",
            "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
            "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('c:/Users/johnn/Desktop/EC ENGR C143A/c143a-project/notebooks/data/librispeech_sentences.pkl')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "RAW_DATA = ROOT / \"data\"\n",
        "BOOKS_DIR = RAW_DATA / \"LibriSpeech\" / \"books\" / \"ascii\"\n",
        "OUTPUT_PATHS = [\n",
        "    RAW_DATA / \"librispeech_sentences.pkl\",\n",
        "    ROOT / \"data\" / \"librispeech_sentences.pkl\",\n",
        "]\n",
        "\n",
        "SENTENCE_RE = re.compile(r\"[A-Za-z][^.!?]*[.!?]\")\n",
        "MIN_WORDS, MAX_WORDS = 3, 50\n",
        "\n",
        "if not BOOKS_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Missing LibriSpeech text at {BOOKS_DIR}\")\n",
        "\n",
        "\n",
        "def extract_sentences(text: str):\n",
        "    for match in SENTENCE_RE.finditer(text):\n",
        "        sentence = match.group().strip()\n",
        "        length = len(sentence.split())\n",
        "        if MIN_WORDS <= length <= MAX_WORDS:\n",
        "            yield sentence\n",
        "\n",
        "\n",
        "sentences = []\n",
        "text_files = sorted(BOOKS_DIR.rglob(\"*.txt\"))\n",
        "print(f\"Scanning {len(text_files)} text files under {BOOKS_DIR} ...\")\n",
        "for txt in text_files:\n",
        "    text = txt.read_text(encoding=\"utf-8\", errors=\"ignore\").replace(\"\\n\", \" \")\n",
        "    sentences.extend(extract_sentences(text))\n",
        "\n",
        "sentences = list(dict.fromkeys(sentences))  # preserve order while deduplicating\n",
        "print(f\"Collected {len(sentences):,} sentences after filtering + deduplication.\")\n",
        "\n",
        "for out in OUTPUT_PATHS:\n",
        "    out.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out.open(\"wb\") as f:\n",
        "        pickle.dump(sentences, f)\n",
        "    print(f\"Saved sentences to {out}\")\n",
        "\n",
        "ROOT / \"data\" / \"librispeech_sentences.pkl\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "493b9fd7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x14f31dfddd0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperparameters / config collected here for quick tuning\n",
        "DATA_DIR = Path(\"data\")\n",
        "CMU_PATH = DATA_DIR / \"cmudict-0.7b\"\n",
        "SENTENCE_PKL = DATA_DIR / \"librispeech_sentences.pkl\"\n",
        "\n",
        "MAX_SENTENCES = 250_000\n",
        "MAX_LEN = 128\n",
        "TRAIN_SAMPLES = 200_000\n",
        "VAL_SAMPLES = 10_000\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 50\n",
        "SEED = 0\n",
        "\n",
        "NOISE = (0.1, 0.1, 0.1)  # (p_sub, p_ins, p_del)\n",
        "D_MODEL = 512\n",
        "N_HEAD = 8\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "DIM_FEEDFORWARD = 2048\n",
        "DROPOUT = 0.2\n",
        "\n",
        "BASE_LR = 1e-2\n",
        "END_LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "WARMUP_PCT = 0.1\n",
        "GRAD_ACCUM_STEPS = 1\n",
        "MIXED_PRECISION = True\n",
        "USE_TF32 = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "rng = random.Random(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e04643be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(134373, 6078025)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load CMUdict and LibriSpeech sentences\n",
        "if not CMU_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing CMUdict at {CMU_PATH}\")\n",
        "if not SENTENCE_PKL.exists():\n",
        "    raise FileNotFoundError(f\"Missing LibriSpeech sentences at {SENTENCE_PKL}\")\n",
        "\n",
        "stress_digits = \"0123456789\"\n",
        "cmu_map: Dict[str, List[str]] = {}\n",
        "with CMU_PATH.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\";;;\"):\n",
        "            continue\n",
        "        parts = line.split()\n",
        "        word, phones = parts[0], parts[1:]\n",
        "        clean = [ph.translate({ord(d): None for d in stress_digits}) for ph in phones]\n",
        "        cmu_map[word.lower()] = clean\n",
        "\n",
        "with SENTENCE_PKL.open(\"rb\") as f:\n",
        "    raw_sentences: List[str] = pickle.load(f)\n",
        "\n",
        "len(cmu_map), len(raw_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ad1476b0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(42, [('AA', 3), ('AE', 4), ('AH', 5), ('AO', 6), ('AW', 7)])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build phoneme vocabulary (pad=0, sos=1, eos=2)\n",
            "ALL_PHONEMES = sorted({ph for phones in cmu_map.values() for ph in phones} | {\"<sil>\"})\n",
        "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2\n",
        "phoneme_to_idx: Dict[str, int] = {ph: i + 3 for i, ph in enumerate(ALL_PHONEMES)}\n",
        "phoneme_to_idx[\"<pad>\"] = PAD_IDX\n",
        "phoneme_to_idx[\"<sos>\"] = SOS_IDX\n",
        "phoneme_to_idx[\"<eos>\"] = EOS_IDX\n",
        "idx_to_phoneme = {i: p for p, i in phoneme_to_idx.items()}\n",
        "\n",
        "vocab_size = len(phoneme_to_idx)\n",
        "\n",
        "CONFUSABLE_GROUPS = [\n",
        "    [\"DH\", \"TH\"],\n",
        "    [\"IH\", \"IY\"],\n",
        "    [\"AH\", \"AA\", \"AE\"],\n",
        "    [\"EH\", \"AE\"],\n",
        "    [\"S\", \"Z\"],\n",
        "    [\"SH\", \"ZH\"],\n",
        "    [\"P\", \"B\"],\n",
        "    [\"T\", \"D\"],\n",
        "    [\"K\", \"G\"],\n",
        "    [\"F\", \"V\"],\n",
        "    [\"CH\", \"JH\"],\n",
        "]\n",
        "neighbor_map: Dict[int, List[int]] = {}\n",
        "for group in CONFUSABLE_GROUPS:\n",
        "    ids = [phoneme_to_idx[p] for p in group if p in phoneme_to_idx]\n",
        "    for pid in ids:\n",
        "        neighbor_map[pid] = [q for q in ids if q != pid]\n",
        "\n",
        "vocab_size, list(phoneme_to_idx.items())[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "53a12379",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 180,231 sentence records (from 250,000 samples).\n"
          ]
        }
      ],
      "source": [
        "WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "def sentence_to_phonemes(text: str) -> Optional[List[str]]:\n",
        "    words = WORD_RE.findall(text.lower())\n",
        "    if not words:\n",
        "        return None\n",
        "    phonemes: List[str] = []\n",
        "    for w in words:\n",
        "        phones = cmu_map.get(w)\n",
        "        if not phones:\n",
        "            return None\n",
        "        phonemes.extend(phones)\n",
        "    return phonemes\n",
        "\n",
        "if MAX_SENTENCES and len(raw_sentences) > MAX_SENTENCES:\n",
        "    sampled_sentences = rng.sample(raw_sentences, MAX_SENTENCES)\n",
        "else:\n",
        "    sampled_sentences = list(raw_sentences)\n",
        "\n",
        "sentence_records: List[Dict[str, object]] = []\n",
        "for sent in sampled_sentences:\n",
        "    phonemes = sentence_to_phonemes(sent)\n",
        "    if phonemes:\n",
        "        sentence_records.append({\"text\": sent, \"phonemes\": phonemes})\n",
        "\n",
        "if not sentence_records:\n",
        "    raise RuntimeError(\"No sentences could be converted with CMUdict coverage.\")\n",
        "print(f\"Prepared {len(sentence_records):,} sentence records (from {len(sampled_sentences):,} samples).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "df07a3e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "noisy:  ['AA', 'N', 'D', 'R', 'EY', 'D', 'EH', 'S', 'AA', 'R', 'T', '<eos>', 'OY']\n",
            "clean:  ['AA', 'N', 'D', 'R', 'EY', 'D', 'EH', 'S', 'AA', 'R', 'T', 'S', '<eos>']\n"
          ]
        }
      ],
      "source": [
        "def add_noise(\n",
        "    seq: List[int],\n",
        "    vocab_size: int,\n",
        "    p_sub: float = 0.1,\n",
        "    p_ins: float = 0.05,\n",
        "    p_del: float = 0.05,\n",
        ") -> List[int]:\n",
        "    noisy: List[int] = []\n",
        "    for token in seq:\n",
        "        if random.random() < p_del:\n",
        "            continue\n",
        "        if random.random() < p_sub:\n",
        "            neighbors = neighbor_map.get(token, [])\n",
        "            if neighbors and random.random() < 0.8:\n",
        "                token = random.choice(neighbors)\n",
        "            else:\n",
        "                token = random.randint(3, vocab_size - 1)\n",
        "        noisy.append(token)\n",
        "        if random.random() < p_ins:\n",
        "            noisy.append(random.randint(3, vocab_size - 1))\n",
        "    return noisy\n",
        "\n",
        "\n",
        "def encode_phonemes(phonemes: List[str], max_len: int) -> List[int]:\n",
        "    tokens = [phoneme_to_idx[p] for p in phonemes if p in phoneme_to_idx]\n",
        "    tokens = tokens[: max_len - 2]\n",
        "    tokens.append(EOS_IDX)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def pad_sequence(seq: List[int], max_len: int, pad_value: int = PAD_IDX) -> List[int]:\n",
        "    return seq + [pad_value] * (max_len - len(seq))\n",
        "\n",
        "\n",
        "class PhonemeCorrectionDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        records: List[Dict[str, object]],\n",
        "        max_len: int = MAX_LEN,\n",
        "        num_samples: Optional[int] = None,\n",
        "        noise: Tuple[float, float, float] = NOISE,\n",
        "    ) -> None:\n",
        "        self.records = records\n",
        "        self.max_len = max_len\n",
        "        self.num_samples = num_samples or len(records)\n",
        "        self.noise = noise\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        clean_ph: List[str] = self.records[idx % len(self.records)][\"phonemes\"]  # type: ignore[index]\n",
        "        tgt_tokens = encode_phonemes(clean_ph, self.max_len)\n",
        "        noisy_tokens = add_noise(\n",
        "            tgt_tokens,\n",
        "            vocab_size,\n",
        "            p_sub=self.noise[0],\n",
        "            p_ins=self.noise[1],\n",
        "            p_del=self.noise[2],\n",
        "        )\n",
        "        noisy_tokens = noisy_tokens[: self.max_len]\n",
        "        tgt_tokens = tgt_tokens[: self.max_len]\n",
        "        return torch.tensor(noisy_tokens), torch.tensor(tgt_tokens)\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    max_src = min(max(len(s) for s in srcs), MAX_LEN)\n",
        "    max_tgt = min(max(len(t) for t in tgts) + 2, MAX_LEN)  # room for <sos>/<eos>\n",
        "\n",
        "    padded_src = [pad_sequence(s.tolist()[:max_src], max_src) for s in srcs]\n",
        "    tgt_in, tgt_out = [], []\n",
        "    for tgt in tgts:\n",
        "        tokens = tgt.tolist()[: max_tgt - 2]\n",
        "        seq = [SOS_IDX] + tokens + [EOS_IDX]\n",
        "        seq = seq[:max_tgt]\n",
        "        seq += [PAD_IDX] * (max_tgt - len(seq))\n",
        "        tgt_in.append(seq[:-1])\n",
        "        tgt_out.append(seq[1:])\n",
        "\n",
        "    return torch.tensor(padded_src), torch.tensor(tgt_in), torch.tensor(tgt_out)\n",
        "\n",
        "\n",
        "# quick sanity check\n",
        "_ds = PhonemeCorrectionDataset(sentence_records, max_len=32, num_samples=5)\n",
        "for noisy, clean in _ds:\n",
        "    print(\"noisy: \", [idx_to_phoneme[i] for i in noisy.tolist() if i != PAD_IDX])\n",
        "    print(\"clean: \", [idx_to_phoneme[i] for i in clean.tolist() if i != PAD_IDX])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5232f063",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000) -> None:\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.pe[:, : x.size(1)].to(x.device)\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "    mask = torch.full((sz, sz), float('-inf'), device=device)\n",
        "    mask = torch.triu(mask, diagonal=1)\n",
        "    mask.fill_diagonal_(0.0)\n",
        "    return mask\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = D_MODEL,\n",
        "        nhead: int = N_HEAD,\n",
        "        num_encoder_layers: int = NUM_ENCODER_LAYERS,\n",
        "        num_decoder_layers: int = NUM_DECODER_LAYERS,\n",
        "        dim_feedforward: int = DIM_FEEDFORWARD,\n",
        "        dropout: float = DROPOUT,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.src_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
        "        self.tgt_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor) -> torch.Tensor:\n",
        "        src_key_padding_mask = src == PAD_IDX\n",
        "        tgt_key_padding_mask = tgt_in == PAD_IDX\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt_in.size(1), tgt_in.device)\n",
        "\n",
        "        src_h = self.encoder(self.pos_enc(self.src_emb(src)), src_key_padding_mask=src_key_padding_mask)\n",
        "        tgt_h = self.decoder(\n",
        "            self.pos_enc(self.tgt_emb(tgt_in)),\n",
        "            src_h,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask,\n",
        "        )\n",
        "        return self.proj(tgt_h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a7211a6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_82724\\917891412.py:43: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=MIXED_PRECISION and DEVICE.type == \"cuda\")\n"
          ]
        }
      ],
      "source": [
        "# Training setup\n",
        "train_samples = min(TRAIN_SAMPLES, len(sentence_records))\n",
        "val_samples = min(VAL_SAMPLES, len(sentence_records))\n",
        "\n",
        "train_ds = PhonemeCorrectionDataset(sentence_records, max_len=MAX_LEN, num_samples=train_samples, noise=NOISE)\n",
        "val_ds = PhonemeCorrectionDataset(sentence_records, max_len=MAX_LEN, num_samples=val_samples, noise=NOISE)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=D_MODEL,\n",
        "    nhead=N_HEAD,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "if DEVICE.type == \"cuda\" and USE_TF32:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
        "total_train_iters = max(1, NUM_EPOCHS * len(train_loader))\n",
        "warmup_iters = max(1, int(WARMUP_PCT * total_train_iters))\n",
        "min_lr_factor = END_LR / BASE_LR\n",
        "\n",
        "\n",
        "def lr_lambda(step: int) -> float:\n",
        "    if step < warmup_iters:\n",
        "        return (step + 1) / warmup_iters\n",
        "    remaining = max(total_train_iters - warmup_iters, 1)\n",
        "    decay_step = step - warmup_iters\n",
        "    decay_frac = max(0.0, 1.0 - decay_step / remaining)\n",
        "    return max(min_lr_factor, decay_frac)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "scaler = GradScaler(enabled=MIXED_PRECISION and DEVICE.type == \"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4470b77d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_82724\\3875417296.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler.is_enabled()):\n",
            "c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 79\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m---> 79\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[12], line 67\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accum\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     do_step \u001b[38;5;241m=\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accum \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m num_batches\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_step:\n",
            "File \u001b[1;32mc:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def tokens_to_phonemes(tokens: List[int]) -> List[str]:\n",
        "    return [idx_to_phoneme.get(int(t), f\"<{t}>\") for t in tokens if int(t) not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
        "\n",
        "\n",
        "def greedy_decode(src: torch.Tensor, max_len: int = MAX_LEN) -> List[int]:\n",
        "    model.eval()\n",
        "    src = src.unsqueeze(0).to(DEVICE)\n",
        "    src_key_padding_mask = src == PAD_IDX\n",
        "    memory = model.encoder(model.pos_enc(model.src_emb(src)), src_key_padding_mask=src_key_padding_mask)\n",
        "    ys = torch.tensor([[SOS_IDX]], device=DEVICE)\n",
        "    for _ in range(max_len):\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.size(1), ys.device)\n",
        "        out = model.decoder(\n",
        "            model.pos_enc(model.tgt_emb(ys)),\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=None,\n",
        "            memory_key_padding_mask=src_key_padding_mask,\n",
        "        )\n",
        "        logits = model.proj(out[:, -1, :])\n",
        "        next_word = logits.argmax(dim=-1).unsqueeze(0)\n",
        "        ys = torch.cat([ys, next_word], dim=1)\n",
        "        if next_word.item() == EOS_IDX:\n",
        "            break\n",
        "    return ys.squeeze(0).tolist()\n",
        "\n",
        "\n",
        "def log_val_example(example_idx: int = 0) -> None:\n",
        "    model.eval()\n",
        "    noisy, tgt = val_ds[example_idx % len(val_ds)]\n",
        "    rec = val_ds.records[example_idx % len(val_ds)]\n",
        "    text = rec.get('text', '') if isinstance(rec, dict) else ''\n",
        "    pred_tokens = greedy_decode(noisy, max_len=MAX_LEN)\n",
        "\n",
        "    noisy_ph = tokens_to_phonemes(noisy.tolist())\n",
        "    tgt_ph = tokens_to_phonemes(tgt.tolist())\n",
        "    pred_ph = tokens_to_phonemes(pred_tokens)\n",
        "\n",
        "    print(f\"[val example] noisy:        {' '.join(noisy_ph)}\")\n",
        "    print(f\"[val example] target:       {' '.join(tgt_ph)}\")\n",
        "    print(f\"[val example] corrected:    {' '.join(pred_ph) if pred_ph else '<empty>'}\")\n",
        "    if text:\n",
        "        print(f\"[val example] plain text:   {text}\")\n",
        "\n",
        "\n",
        "def run_epoch(loader, train: bool = True):\n",
        "    model.train(train)\n",
        "    total_loss = 0.0\n",
        "    accum = GRAD_ACCUM_STEPS if train else 1\n",
        "    if train:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    num_batches = len(loader)\n",
        "\n",
        "    for step, (src, tgt_in, tgt_out) in enumerate(loader):\n",
        "        src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
        "\n",
        "        # autocast may not accept a device_type kwarg on all torch versions;\n",
        "        # use enabled=scaler.is_enabled() which already reflects MIXED_PRECISION & CUDA\n",
        "        with autocast(enabled=scaler.is_enabled()):\n",
        "            logits = model(src, tgt_in)\n",
        "            loss = criterion(logits.reshape(-1, vocab_size), tgt_out.reshape(-1))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if train:\n",
        "            loss = loss / accum\n",
        "            scaler.scale(loss).backward()\n",
        "            do_step = (step + 1) % accum == 0 or (step + 1) == num_batches\n",
        "            if do_step:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                scheduler.step()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = run_epoch(train_loader, train=True)\n",
        "    val_loss = run_epoch(val_loader, train=False)\n",
        "    print(f\"epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | lr={scheduler.get_last_lr()[0]:.6f}\")\n",
        "    log_val_example(example_idx=epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbe54d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decode a few examples (greedy autoregressive)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    src_batch, tgt_batch, _ = next(iter(val_loader))\n",
        "    src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
        "\n",
        "for i in range(3):\n",
        "    noisy_tokens = [idx_to_phoneme[t.item()] for t in src_batch[i] if t.item() != PAD_IDX]\n",
        "    clean_tokens = [idx_to_phoneme[t.item()] for t in tgt_batch[i] if t.item() not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
        "    pred_tokens = greedy_decode(src_batch[i].cpu(), max_len=MAX_LEN)\n",
        "    pred = [idx_to_phoneme.get(int(t), f\"<{t}>\") for t in pred_tokens if int(t) not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
        "    print(f\"Noisy    : {' '.join(noisy_tokens)}\")\n",
        "    print(f\"Target   : {' '.join(clean_tokens)}\")\n",
        "    print(f\"Pred     : {' '.join(pred)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50833655",
      "metadata": {},
      "outputs": [],
      "source": [
        "export_path = Path(\"/phoneme_seq2seq.pt\")\n",
        "export_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "checkpoint = {\n",
        "    \"state_dict\": model.state_dict(),\n",
        "    \"phoneme_to_idx\": phoneme_to_idx,\n",
        "    \"idx_to_phoneme\": idx_to_phoneme,\n",
        "    \"pad_idx\": PAD_IDX,\n",
        "    \"sos_idx\": SOS_IDX,\n",
        "    \"eos_idx\": EOS_IDX,\n",
        "    \"max_len\": MAX_LEN,\n",
        "    \"model_kwargs\": {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"d_model\": D_MODEL,\n",
        "        \"nhead\": N_HEAD,\n",
        "        \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
        "        \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
        "        \"dim_feedforward\": DIM_FEEDFORWARD,\n",
        "        \"dropout\": DROPOUT,\n",
        "    },\n",
        "}\n",
        "torch.save(checkpoint, export_path)\n",
        "print(f\"Saved phoneme seq2seq checkpoint to {export_path.resolve()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "c143a-project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
