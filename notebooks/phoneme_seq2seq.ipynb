{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990194a9",
   "metadata": {},
   "source": [
    "# Phoneme seq2seq with CMUdict (encoder-decoder)\n",
    "Train a Transformer encoder-decoder with cross-entropy to map noisy phoneme sequences back to clean phoneme sequences using LibriSpeech text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e6a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7fd51",
   "metadata": {},
   "source": [
    "# Rebuild LibriSpeech sentences\n",
    "Use LibriSpeech text in notebooks/data to regenerate librispeech_sentences.pkl into notebooks/data and data/ before loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21902fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 1443 text files under c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\LibriSpeech\\books\\ascii ...\n",
      "Collected 6,078,025 sentences after filtering + deduplication.\n",
      "Collected 6,078,025 sentences after filtering + deduplication.\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/johnn/Desktop/EC ENGR C143A/c143a-project/notebooks/data/librispeech_sentences.pkl')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "RAW_DATA = ROOT / \"data\"\n",
    "BOOKS_DIR = RAW_DATA / \"LibriSpeech\" / \"books\" / \"ascii\"\n",
    "OUTPUT_PATHS = [\n",
    "    RAW_DATA / \"librispeech_sentences.pkl\",\n",
    "    ROOT / \"data\" / \"librispeech_sentences.pkl\",\n",
    "]\n",
    "\n",
    "SENTENCE_RE = re.compile(r\"[A-Za-z][^.!?]*[.!?]\")\n",
    "MIN_WORDS, MAX_WORDS = 3, 50\n",
    "\n",
    "if not BOOKS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing LibriSpeech text at {BOOKS_DIR}\")\n",
    "\n",
    "\n",
    "def extract_sentences(text: str):\n",
    "    for match in SENTENCE_RE.finditer(text):\n",
    "        sentence = match.group().strip()\n",
    "        length = len(sentence.split())\n",
    "        if MIN_WORDS <= length <= MAX_WORDS:\n",
    "            yield sentence\n",
    "\n",
    "\n",
    "sentences = []\n",
    "text_files = sorted(BOOKS_DIR.rglob(\"*.txt\"))\n",
    "print(f\"Scanning {len(text_files)} text files under {BOOKS_DIR} ...\")\n",
    "for txt in text_files:\n",
    "    text = txt.read_text(encoding=\"utf-8\", errors=\"ignore\").replace(\"\\n\", \" \")\n",
    "    sentences.extend(extract_sentences(text))\n",
    "\n",
    "sentences = list(dict.fromkeys(sentences))  # preserve order while deduplicating\n",
    "print(f\"Collected {len(sentences):,} sentences after filtering + deduplication.\")\n",
    "\n",
    "for out in OUTPUT_PATHS:\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out.open(\"wb\") as f:\n",
    "        pickle.dump(sentences, f)\n",
    "    print(f\"Saved sentences to {out}\")\n",
    "\n",
    "ROOT / \"data\" / \"librispeech_sentences.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "493b9fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f13221ddd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters / config collected here for quick tuning\n",
    "DATA_DIR = Path(\"data\")\n",
    "CMU_PATH = DATA_DIR / \"cmudict-0.7b\"\n",
    "SENTENCE_PKL = DATA_DIR / \"librispeech_sentences.pkl\"\n",
    "\n",
    "MAX_SENTENCES = 6_000_000\n",
    "MAX_LEN = 128\n",
    "TRAIN_SAMPLES = 4_000_000\n",
    "VAL_SAMPLES = 10_000\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 10\n",
    "SEED = 0\n",
    "\n",
    "NOISE = (0.1, 0.1, 0.1)  # (p_sub, p_ins, p_del)\n",
    "D_MODEL = 256\n",
    "N_HEAD = 8\n",
    "BIDIRECTIONAL = False  # False = streaming (causal); True = limited lookahead (current + next phoneme)\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "DIM_FEEDFORWARD = 1024\n",
    "DROPOUT = 0.2\n",
    "\n",
    "BASE_LR = 1e-3\n",
    "END_LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_PCT = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "MIXED_PRECISION = True\n",
    "USE_TF32 = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rng = random.Random(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04643be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134373, 6078025)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CMUdict and LibriSpeech sentences\n",
    "if not CMU_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing CMUdict at {CMU_PATH}\")\n",
    "if not SENTENCE_PKL.exists():\n",
    "    raise FileNotFoundError(f\"Missing LibriSpeech sentences at {SENTENCE_PKL}\")\n",
    "\n",
    "stress_digits = \"0123456789\"\n",
    "cmu_map: Dict[str, List[List[str]]] = {}\n",
    "with CMU_PATH.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\";;;\"):\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        word, phones = parts[0], parts[1:]\n",
    "        clean = [ph.translate({ord(d): None for d in stress_digits}) for ph in phones]\n",
    "        cmu_map.setdefault(word.lower(), []).append(clean)\n",
    "\n",
    "with SENTENCE_PKL.open(\"rb\") as f:\n",
    "    raw_sentences: List[str] = pickle.load(f)\n",
    "\n",
    "len(cmu_map), len(raw_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1476b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme vocab (40): AA, AE, AH, AO, AW, AY, B, CH, D, DH, EH, ER, EY, F, G, HH, IH, IY, JH, K, L, M, N, NG, OW, OY, P, R, S, SH, T, TH, UH, UW, V, W, Y, Z, ZH, <sil>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, [('AA', 3), ('AE', 4), ('AH', 5), ('AO', 6), ('AW', 7)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build phoneme vocabulary (pad=0, sos=1, eos=2)\n",
    "ALL_PHONEMES = sorted(\n",
    "    set(ph for word_prons in cmu_map.values() for phoneme_seq in word_prons for ph in phoneme_seq)\n",
    ")\n",
    "if \"<sil>\" not in ALL_PHONEMES:\n",
    "    ALL_PHONEMES.append(\"<sil>\")\n",
    "else:\n",
    "    ALL_PHONEMES = [ph for ph in ALL_PHONEMES if ph != \"<sil>\"] + [\"<sil>\"]\n",
    "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2\n",
    "phoneme_to_idx: Dict[str, int] = {ph: i + 3 for i, ph in enumerate(ALL_PHONEMES)}\n",
    "phoneme_to_idx[\"<pad>\"] = PAD_IDX\n",
    "phoneme_to_idx[\"<sos>\"] = SOS_IDX\n",
    "phoneme_to_idx[\"<eos>\"] = EOS_IDX\n",
    "idx_to_phoneme = {i: p for p, i in phoneme_to_idx.items()}\n",
    "\n",
    "print(f\"Phoneme vocab ({len(ALL_PHONEMES)}): {', '.join(ALL_PHONEMES)}\")\n",
    "\n",
    "vocab_size = len(phoneme_to_idx)\n",
    "\n",
    "CONFUSABLE_GROUPS = [\n",
    "    [\"DH\", \"TH\"],\n",
    "    [\"IH\", \"IY\"],\n",
    "    [\"AH\", \"AA\", \"AE\"],\n",
    "    [\"EH\", \"AE\"],\n",
    "    [\"S\", \"Z\"],\n",
    "    [\"SH\", \"ZH\"],\n",
    "    [\"P\", \"B\"],\n",
    "    [\"T\", \"D\"],\n",
    "    [\"K\", \"G\"],\n",
    "    [\"F\", \"V\"],\n",
    "    [\"CH\", \"JH\"],\n",
    "]\n",
    "neighbor_map: Dict[int, List[int]] = {}\n",
    "for group in CONFUSABLE_GROUPS:\n",
    "    ids = [phoneme_to_idx[p] for p in group if p in phoneme_to_idx]\n",
    "    for pid in ids:\n",
    "        neighbor_map[pid] = [q for q in ids if q != pid]\n",
    "\n",
    "vocab_size, list(phoneme_to_idx.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a12379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 4,315,739 sentence records (from 6,000,000 samples).\n"
     ]
    }
   ],
   "source": [
    "WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "def sentence_to_phonemes(text: str) -> Optional[List[str]]:\n",
    "    words = WORD_RE.findall(text.lower())\n",
    "    if not words:\n",
    "        return None\n",
    "    phonemes: List[str] = []\n",
    "    for i, w in enumerate(words):\n",
    "        prons = cmu_map.get(w)\n",
    "        if not prons:\n",
    "            return None\n",
    "        phones = rng.choice(prons)\n",
    "        phonemes.extend(phones)\n",
    "        if i < len(words) - 1:\n",
    "            phonemes.append(\"<sil>\")\n",
    "    return phonemes\n",
    "\n",
    "if MAX_SENTENCES and len(raw_sentences) > MAX_SENTENCES:\n",
    "    sampled_sentences = rng.sample(raw_sentences, MAX_SENTENCES)\n",
    "else:\n",
    "    sampled_sentences = list(raw_sentences)\n",
    "\n",
    "sentence_records: List[Dict[str, object]] = []\n",
    "for sent in sampled_sentences:\n",
    "    phonemes = sentence_to_phonemes(sent)\n",
    "    if phonemes:\n",
    "        sentence_records.append({\"text\": sent, \"phonemes\": phonemes})\n",
    "\n",
    "if not sentence_records:\n",
    "    raise RuntimeError(\"No sentences could be converted with CMUdict coverage.\")\n",
    "print(f\"Prepared {len(sentence_records):,} sentence records (from {len(sampled_sentences):,} samples).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df07a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy:  ['AA', 'P', 'D', 'EY', 'NG', '<sil>', 'D', 'EH', 'S', '<sil>', 'AA', 'R', 'T', 'S', '<eos>']\n",
      "clean:  ['AA', 'N', 'D', 'R', 'EY', '<sil>', 'D', 'EH', 'S', '<sil>', 'AA', 'R', 'T', 'S', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def add_noise(\n",
    "    seq: List[int],\n",
    "    vocab_size: int,\n",
    "    p_sub: float = 0.1,\n",
    "    p_ins: float = 0.05,\n",
    "    p_del: float = 0.05,\n",
    ") -> List[int]:\n",
    "    noisy: List[int] = []\n",
    "    for token in seq:\n",
    "        if random.random() < p_del:\n",
    "            continue\n",
    "        if random.random() < p_sub:\n",
    "            neighbors = neighbor_map.get(token, [])\n",
    "            if neighbors and random.random() < 0.8:\n",
    "                token = random.choice(neighbors)\n",
    "            else:\n",
    "                token = random.randint(3, vocab_size - 1)\n",
    "        noisy.append(token)\n",
    "        if random.random() < p_ins:\n",
    "            noisy.append(random.randint(3, vocab_size - 1))\n",
    "    return noisy\n",
    "\n",
    "\n",
    "def encode_phonemes(phonemes: List[str], max_len: int) -> List[int]:\n",
    "    tokens = [phoneme_to_idx[p] for p in phonemes if p in phoneme_to_idx]\n",
    "    tokens = tokens[: max_len - 2]\n",
    "    tokens.append(EOS_IDX)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def pad_sequence(seq: List[int], max_len: int, pad_value: int = PAD_IDX) -> List[int]:\n",
    "    return seq + [pad_value] * (max_len - len(seq))\n",
    "\n",
    "\n",
    "class PhonemeCorrectionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        records: List[Dict[str, object]],\n",
    "        max_len: int = MAX_LEN,\n",
    "        num_samples: Optional[int] = None,\n",
    "        noise: Tuple[float, float, float] = NOISE,\n",
    "    ) -> None:\n",
    "        self.records = records\n",
    "        self.max_len = max_len\n",
    "        self.num_samples = num_samples or len(records)\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        clean_ph: List[str] = self.records[idx % len(self.records)][\"phonemes\"]  # type: ignore[index]\n",
    "        tgt_tokens = encode_phonemes(clean_ph, self.max_len)\n",
    "        noisy_tokens = add_noise(\n",
    "            tgt_tokens,\n",
    "            vocab_size,\n",
    "            p_sub=self.noise[0],\n",
    "            p_ins=self.noise[1],\n",
    "            p_del=self.noise[2],\n",
    "        )\n",
    "        noisy_tokens = noisy_tokens[: self.max_len]\n",
    "        tgt_tokens = tgt_tokens[: self.max_len]\n",
    "        return torch.tensor(noisy_tokens), torch.tensor(tgt_tokens)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    srcs, tgts = zip(*batch)\n",
    "    max_src = min(max(len(s) for s in srcs), MAX_LEN)\n",
    "    max_tgt = min(max(len(t) for t in tgts) + 2, MAX_LEN)  # room for <sos>/<eos>\n",
    "\n",
    "    padded_src = [pad_sequence(s.tolist()[:max_src], max_src) for s in srcs]\n",
    "    tgt_in, tgt_out = [], []\n",
    "    for tgt in tgts:\n",
    "        tokens = tgt.tolist()[: max_tgt - 2]\n",
    "        seq = [SOS_IDX] + tokens + [EOS_IDX]\n",
    "        seq = seq[:max_tgt]\n",
    "        seq += [PAD_IDX] * (max_tgt - len(seq))\n",
    "        tgt_in.append(seq[:-1])\n",
    "        tgt_out.append(seq[1:])\n",
    "\n",
    "    return torch.tensor(padded_src), torch.tensor(tgt_in), torch.tensor(tgt_out)\n",
    "\n",
    "\n",
    "# quick sanity check\n",
    "_ds = PhonemeCorrectionDataset(sentence_records, max_len=32, num_samples=5)\n",
    "for noisy, clean in _ds:\n",
    "    print(\"noisy: \", [idx_to_phoneme[i] for i in noisy.tolist() if i != PAD_IDX])\n",
    "    print(\"clean: \", [idx_to_phoneme[i] for i in clean.tolist() if i != PAD_IDX])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, : x.size(1)].to(x.device)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
    "    mask = torch.full((sz, sz), float('-inf'), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    mask.fill_diagonal_(0.0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_limited_future_mask(sz: int, device: torch.device, lookahead: int = 1) -> torch.Tensor:\n",
    "    # Allow attention to current position and up to `lookahead` future steps; disallow further lookahead.\n",
    "    i = torch.arange(sz, device=device).unsqueeze(1)\n",
    "    j = torch.arange(sz, device=device)S\n",
    "    return torch.where(j - i > lookahead, float('-inf'), 0.0)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = D_MODEL,\n",
    "        nhead: int = N_HEAD,\n",
    "        num_encoder_layers: int = NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers: int = NUM_DECODER_LAYERS,\n",
    "        dim_feedforward: int = DIM_FEEDFORWARD,\n",
    "        dropout: float = DROPOUT,\n",
    "        bidirectional: bool = BIDIRECTIONAL,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.src_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.tgt_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def encode(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        src_key_padding_mask = src == PAD_IDX\n",
    "        if self.bidirectional:\n",
    "            src_mask = generate_limited_future_mask(src.size(1), src.device, lookahead=1)\n",
    "        else:\n",
    "            src_mask = generate_square_subsequent_mask(src.size(1), src.device)\n",
    "        return self.encoder(\n",
    "            self.pos_enc(self.src_emb(src)),\n",
    "            mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor) -> torch.Tensor:\n",
    "        src_key_padding_mask = src == PAD_IDX\n",
    "        tgt_key_padding_mask = tgt_in == PAD_IDX\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_in.size(1), tgt_in.device)\n",
    "\n",
    "        src_h = self.encode(src)\n",
    "        tgt_h = self.decoder(\n",
    "            self.pos_enc(self.tgt_emb(tgt_in)),\n",
    "            src_h,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "        return self.proj(tgt_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7211a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_67944\\2452248985.py:59: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=MIXED_PRECISION and DEVICE.type == \"cuda\")\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "train_samples = min(TRAIN_SAMPLES, len(sentence_records))\n",
    "val_samples = min(VAL_SAMPLES, max(0, len(sentence_records) - train_samples))\n",
    "\n",
    "# explicit train/val split with deterministic shuffle\n",
    "split_records = list(sentence_records)\n",
    "rng.shuffle(split_records)\n",
    "train_records = split_records[:train_samples]\n",
    "val_records = split_records[train_samples: train_samples + val_samples] or split_records[-val_samples:]\n",
    "\n",
    "train_ds = PhonemeCorrectionDataset(train_records, max_len=MAX_LEN, num_samples=len(train_records), noise=NOISE)\n",
    "val_ds = PhonemeCorrectionDataset(val_records, max_len=MAX_LEN, num_samples=len(val_records), noise=NOISE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    ")\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "if DEVICE.type == \"cuda\" and USE_TF32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "total_train_iters = max(1, NUM_EPOCHS * len(train_loader))\n",
    "warmup_iters = max(1, int(WARMUP_PCT * total_train_iters))\n",
    "min_lr_factor = END_LR / BASE_LR\n",
    "\n",
    "\n",
    "def lr_lambda(step: int) -> float:\n",
    "    if step < warmup_iters:\n",
    "        return (step + 1) / warmup_iters\n",
    "    remaining = max(total_train_iters - warmup_iters, 1)\n",
    "    decay_step = step - warmup_iters\n",
    "    decay_frac = max(0.0, 1.0 - decay_step / remaining)\n",
    "    return max(min_lr_factor, decay_frac)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = GradScaler(enabled=MIXED_PRECISION and DEVICE.type == \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4470b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_phonemes(tokens: List[int]) -> List[str]:\n",
    "    return [idx_to_phoneme.get(int(t), f\"<{t}>\") for t in tokens if int(t) not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "\n",
    "\n",
    "def greedy_decode(src: torch.Tensor, max_len: int = MAX_LEN) -> List[int]:\n",
    "    model.eval()\n",
    "    src = src.unsqueeze(0).to(DEVICE)\n",
    "    src_key_padding_mask = src == PAD_IDX\n",
    "    memory = model.encode(src)\n",
    "    ys = torch.tensor([[SOS_IDX]], device=DEVICE)\n",
    "    for _ in range(max_len):\n",
    "        tgt_mask = generate_square_subsequent_mask(ys.size(1), ys.device)\n",
    "        out = model.decoder(\n",
    "            model.pos_enc(model.tgt_emb(ys)),\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=None,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "        logits = model.proj(out[:, -1, :])\n",
    "        next_word = logits.argmax(dim=-1).unsqueeze(0)\n",
    "        ys = torch.cat([ys, next_word], dim=1)\n",
    "        if next_word.item() == EOS_IDX:\n",
    "            break\n",
    "    return ys.squeeze(0).tolist()\n",
    "\n",
    "\n",
    "def log_val_example(example_idx: int = 0) -> None:\n",
    "    model.eval()\n",
    "    noisy, tgt = val_ds[example_idx % len(val_ds)]\n",
    "    rec = val_ds.records[example_idx % len(val_ds)]\n",
    "    text = rec.get('text', '') if isinstance(rec, dict) else ''\n",
    "    pred_tokens = greedy_decode(noisy, max_len=MAX_LEN)\n",
    "\n",
    "    noisy_ph = tokens_to_phonemes(noisy.tolist())\n",
    "    tgt_ph = tokens_to_phonemes(tgt.tolist())\n",
    "    pred_ph = tokens_to_phonemes(pred_tokens)\n",
    "\n",
    "    print(f\"[val example] noisy:        {' '.join(noisy_ph)}\")\n",
    "    print(f\"[val example] target:       {' '.join(tgt_ph)}\")\n",
    "    print(f\"[val example] corrected:    {' '.join(pred_ph) if pred_ph else '<empty>'}\")\n",
    "    if text:\n",
    "        print(f\"[val example] plain text:   {text}\")\n",
    "\n",
    "\n",
    "def run_epoch(loader, train: bool = True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    accum = GRAD_ACCUM_STEPS if train else 1\n",
    "    if train:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for step, (src, tgt_in, tgt_out) in enumerate(loader):\n",
    "        src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
    "\n",
    "        # autocast may not accept a device_type kwarg on all torch versions;\n",
    "        # use enabled=scaler.is_enabled() which already reflects MIXED_PRECISION & CUDA\n",
    "        with autocast(enabled=scaler.is_enabled()):\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), tgt_out.reshape(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if train:\n",
    "            loss = loss / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            do_step = (step + 1) % accum == 0 or (step + 1) == num_batches\n",
    "            if do_step:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e702f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    print(f\"epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | lr={scheduler.get_last_lr()[0]:.6f}\")\n",
    "    log_val_example(example_idx=epoch)\n",
    "\n",
    "    # checkpoint every epoch\n",
    "    ckpt_path = Path(\"checkpoints\") / f\"phoneme_seq2seq_epoch_{epoch+1}.pt\"\n",
    "    ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"phoneme_to_idx\": phoneme_to_idx,\n",
    "            \"idx_to_phoneme\": idx_to_phoneme,\n",
    "            \"pad_idx\": PAD_IDX,\n",
    "            \"sos_idx\": SOS_IDX,\n",
    "            \"eos_idx\": EOS_IDX,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"model_kwargs\": {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"d_model\": D_MODEL,\n",
    "                \"nhead\": N_HEAD,\n",
    "                \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
    "                \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
    "                \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "                \"dropout\": DROPOUT,\n",
    "            },\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "    print(f\"Saved epoch {epoch+1} checkpoint to {ckpt_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd0861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_67944\\2411746373.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler.is_enabled()):\n",
      "c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: train_loss=0.2997 | val_loss=0.2427 | lr=0.001000\n",
      "[val example] noisy:        SH UW <sil> UW <sil> D N T <sil> S EY UH S M OW\n",
      "[val example] target:       SH UW <sil> Y UW <sil> D OW N T <sil> S EY <sil> S OW\n",
      "[val example] corrected:    Y UW <sil> D UW <sil> N AA T <sil> S EY <sil> S OW\n",
      "[val example] plain text:   Shoo--you don't say so!\n",
      "Saved epoch 6 checkpoint to C:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\checkpoints\\phoneme_seq2seq_epoch_6.pt\n",
      "epoch 6: train_loss=0.2913 | val_loss=0.2358 | lr=0.000889\n",
      "[val example] noisy:        DH AH <sil> S AY L AH N S <sil> W AA K Z <sil> B IH K AH M IH NG <sil> AH N B EH JH B AH L <sil> SH IY OY <sil> IH S T R AH G AH L D <sil> P UW <sil> TH IH NG K <sil> AH V <sil> S AH M TH NG <sil> T UW <sil> S EY <sil> B AH T <sil> AH IH NG <sil> K M <sil> AH N D <sil> IY <sil> R OW Z <sil> AH AH P L IY\n",
      "[val example] target:       DH AH <sil> S AY L AH N S <sil> W AA Z <sil> B IH K AH M IH NG <sil> AH N B EH R AH B AH L <sil> SH IY <sil> S T R AH G AH L D <sil> T UW <sil> TH IH NG K <sil> AH V <sil> S AH M TH IH NG <sil> T UW <sil> S EY <sil> B AH T <sil> N AH TH IH NG <sil> K EY M <sil> AH N D <sil> SH IY <sil> R OW Z <sil> AH B R AH P T L IY\n",
      "[val example] corrected:    DH AH <sil> S AY L AH N S <sil> W AA Z <sil> B IH K AH M IH NG <sil> AH N B EH R AH B AH L <sil> SH IY <sil> IH N S T R AH K T AH D <sil> T UW <sil> TH IH NG K <sil> AH V <sil> S AH M TH IH NG <sil> T UW <sil> S EY <sil> B AH T <sil> N AH TH IH NG <sil> K EY M <sil> AH N D <sil> SH IY <sil> R OW Z <sil> AH S AH M P L IY\n",
      "[val example] plain text:   The silence was becoming unbearable; she struggled to think of something to say, but nothing came, and she rose abruptly.\n",
      "Saved epoch 7 checkpoint to C:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\checkpoints\\phoneme_seq2seq_epoch_7.pt\n",
      "epoch 7: train_loss=0.2868 | val_loss=0.2364 | lr=0.000778\n",
      "[val example] noisy:        DH <sil> AO L <sil> M N T AH CH <sil> Y UW IH OW S AE <sil> M UH R TH <sil> W AA UH Z <sil> AO AH N AH V <sil> IH Z <sil> M ZH N <sil> V TH N OY D <sil> HH IY <sil> P R NG V V DH D <sil> T UW <sil> IY D <sil> P IH N S <sil> AH V <sil> L UW ER Z\n",
      "[val example] target:       DH AH <sil> T AO L <sil> M AW N T AH N <sil> Y UW TH <sil> S EH TH <sil> M UH R <sil> W AA Z <sil> W AH N <sil> AH V <sil> HH IH Z <sil> M EH N <sil> AH N D <sil> HH IY <sil> P R UW V D <sil> T UW <sil> B IY <sil> AH <sil> P R IH N S <sil> AH V <sil> L UW T ER Z\n",
      "[val example] corrected:    DH AH <sil> AO L <sil> M AE N T AH L <sil> Y UW <sil> S EH D <sil> M UH R <sil> W AA Z <sil> W AH N <sil> AH V <sil> HH IH Z <sil> M AY N D <sil> AH N D <sil> HH IY <sil> P R UW V D <sil> T UW <sil> IY T <sil> P R IH N S <sil> AH V <sil> L UW Z\n",
      "[val example] plain text:   The tall mountain youth, Seth Moore, was one of his men, and he proved to be a prince of looters.\n",
      "Saved epoch 8 checkpoint to C:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\checkpoints\\phoneme_seq2seq_epoch_8.pt\n",
      "epoch 8: train_loss=0.2856 | val_loss=0.2330 | lr=0.000667\n",
      "[val example] noisy:        AA N <sil> L IY V IH NG <sil> DH AH <sil> T IH <sil> SH AA P <sil> AY <sil> W S AO K EY T AH P <sil> S L OW S T R IY T <sil> AO R IH DH <sil> AH <sil> IH F N T EH M CH AH V <sil> AH V D UW IH P UW <sil> W AH T <sil> AY <sil> AO T AH <sil> T UW <sil> HH V D AH N EH ER IY ER <sil> IH N <sil> DH <sil> D M EY\n",
      "[val example] target:       AA N <sil> L IY V IH NG <sil> DH AH <sil> T IY <sil> SH AA P <sil> AY <sil> W AO K T <sil> AH P <sil> S L OW N <sil> S T R IY T <sil> W IH DH <sil> DH AH <sil> IH N T EH N CH AH N <sil> AH V <sil> D UW IH NG <sil> W AH T <sil> AY <sil> AO T <sil> T UW <sil> HH AE V <sil> D AH N <sil> ER L IY ER <sil> IH N <sil> DH AH <sil> D EY\n",
      "[val example] corrected:    AA N <sil> L IY V IH NG <sil> DH AH <sil> T IH P <sil> SH AA P <sil> AY <sil> W AO K T <sil> AH P <sil> S L OW <sil> S T R IY T <sil> W IH DH <sil> AH <sil> IH N T EH N CH AH N <sil> AH V <sil> D UW IH NG <sil> W AH T <sil> AY <sil> AO T <sil> T UW <sil> HH AE V <sil> D AH N <sil> EH V ER IY <sil> IH N <sil> DH AH <sil> D EY\n",
      "[val example] plain text:   On leaving the tea-shop, I walked up Sloane Street with the intention of doing what I ought to have done earlier in the day.\n",
      "Saved epoch 9 checkpoint to C:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\checkpoints\\phoneme_seq2seq_epoch_9.pt\n",
      "epoch 9: train_loss=0.2852 | val_loss=0.2339 | lr=0.000556\n",
      "[val example] noisy:        Y Z JH R IH JH EH K D IY D EH M IY Z <sil> B IH K AO Z <sil> P Y UW <sil> D IH T OY N P AA T <sil> F L <sil> S IH V Y UW UH <sil> DH AH V <sil> M AY R IH N S AH P AH Z\n",
      "[val example] target:       Y UW <sil> R IH JH EH K T IH D <sil> M IY <sil> B IH K AO Z <sil> Y UW <sil> D IH D <sil> N AA T <sil> F IY L <sil> S IH K Y UH R <sil> AH V <sil> M AY <sil> P R IH N S AH P AH L Z\n",
      "[val example] corrected:    Y UW <sil> R IH JH EH K T IH D <sil> M IY <sil> B IH K AO Z <sil> Y UW <sil> D IH D <sil> N AA T <sil> F IY L <sil> S IH K Y UH R <sil> DH AH <sil> M AY <sil> P R IH N S AH P AH L Z\n",
      "[val example] plain text:   You rejected me, because you did not feel secure of my principles.\n",
      "Saved epoch 10 checkpoint to C:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\checkpoints\\phoneme_seq2seq_epoch_10.pt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5, NUM_EPOCHS):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    print(f\"epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | lr={scheduler.get_last_lr()[0]:.6f}\")\n",
    "    log_val_example(example_idx=epoch)\n",
    "\n",
    "    # checkpoint every epoch\n",
    "    ckpt_path = Path(\"checkpoints\") / f\"phoneme_seq2seq_epoch_{epoch+1}.pt\"\n",
    "    ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"phoneme_to_idx\": phoneme_to_idx,\n",
    "            \"idx_to_phoneme\": idx_to_phoneme,\n",
    "            \"pad_idx\": PAD_IDX,\n",
    "            \"sos_idx\": SOS_IDX,\n",
    "            \"eos_idx\": EOS_IDX,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"model_kwargs\": {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"d_model\": D_MODEL,\n",
    "                \"nhead\": N_HEAD,\n",
    "                \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
    "                \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
    "                \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "                \"dropout\": DROPOUT,\n",
    "            },\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "    print(f\"Saved epoch {epoch+1} checkpoint to {ckpt_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c73e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint 5\n",
    "ckpt_path = Path(\"checkpoints\") / \"phoneme_seq2seq_epoch_5.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "BATCH_SIZE = 1024\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe54d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a few examples (greedy autoregressive)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src_batch, tgt_batch, _ = next(iter(val_loader))\n",
    "    src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
    "\n",
    "for i in range(3):\n",
    "    noisy_tokens = [idx_to_phoneme[t.item()] for t in src_batch[i] if t.item() != PAD_IDX]\n",
    "    clean_tokens = [idx_to_phoneme[t.item()] for t in tgt_batch[i] if t.item() not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "    pred_tokens = greedy_decode(src_batch[i].cpu(), max_len=MAX_LEN)\n",
    "    pred = [idx_to_phoneme.get(int(t), f\"<{t}>\") for t in pred_tokens if int(t) not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "    print(f\"Noisy    : {' '.join(noisy_tokens)}\")\n",
    "    print(f\"Target   : {' '.join(clean_tokens)}\")\n",
    "    print(f\"Pred     : {' '.join(pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50833655",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(\"/phoneme_seq2seq.pt\")\n",
    "export_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"phoneme_to_idx\": phoneme_to_idx,\n",
    "    \"idx_to_phoneme\": idx_to_phoneme,\n",
    "    \"pad_idx\": PAD_IDX,\n",
    "    \"sos_idx\": SOS_IDX,\n",
    "    \"eos_idx\": EOS_IDX,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"model_kwargs\": {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"d_model\": D_MODEL,\n",
    "        \"nhead\": N_HEAD,\n",
    "        \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
    "        \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
    "        \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "        \"dropout\": DROPOUT,\n",
    "    },\n",
    "}\n",
    "torch.save(checkpoint, export_path)\n",
    "print(f\"Saved phoneme seq2seq checkpoint to {export_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c143a-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}