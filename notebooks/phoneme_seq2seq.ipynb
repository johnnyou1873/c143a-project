{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990194a9",
   "metadata": {},
   "source": [
    "# Phoneme seq2seq with CMUdict (encoder-decoder)\n",
    "Train a Transformer encoder-decoder with cross-entropy to map noisy phoneme sequences back to clean phoneme sequences using LibriSpeech text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e6a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7fd51",
   "metadata": {},
   "source": [
    "# Rebuild LibriSpeech sentences\n",
    "Use LibriSpeech text in notebooks/data to regenerate librispeech_sentences.pkl into notebooks/data and data/ before loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21902fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 1443 text files under c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\LibriSpeech\\books\\ascii ...\n",
      "Collected 6,078,025 sentences after filtering + deduplication.\n",
      "Collected 6,078,025 sentences after filtering + deduplication.\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n",
      "Saved sentences to c:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\notebooks\\data\\librispeech_sentences.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/johnn/Desktop/EC ENGR C143A/c143a-project/notebooks/data/librispeech_sentences.pkl')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "RAW_DATA = ROOT / \"data\"\n",
    "BOOKS_DIR = RAW_DATA / \"LibriSpeech\" / \"books\" / \"ascii\"\n",
    "OUTPUT_PATHS = [\n",
    "    RAW_DATA / \"librispeech_sentences.pkl\",\n",
    "    ROOT / \"data\" / \"librispeech_sentences.pkl\",\n",
    "]\n",
    "\n",
    "SENTENCE_RE = re.compile(r\"[A-Za-z][^.!?]*[.!?]\")\n",
    "MIN_WORDS, MAX_WORDS = 3, 50\n",
    "\n",
    "if not BOOKS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing LibriSpeech text at {BOOKS_DIR}\")\n",
    "\n",
    "\n",
    "def extract_sentences(text: str):\n",
    "    for match in SENTENCE_RE.finditer(text):\n",
    "        sentence = match.group().strip()\n",
    "        length = len(sentence.split())\n",
    "        if MIN_WORDS <= length <= MAX_WORDS:\n",
    "            yield sentence\n",
    "\n",
    "\n",
    "sentences = []\n",
    "text_files = sorted(BOOKS_DIR.rglob(\"*.txt\"))\n",
    "print(f\"Scanning {len(text_files)} text files under {BOOKS_DIR} ...\")\n",
    "for txt in text_files:\n",
    "    text = txt.read_text(encoding=\"utf-8\", errors=\"ignore\").replace(\"\\n\", \" \")\n",
    "    sentences.extend(extract_sentences(text))\n",
    "\n",
    "sentences = list(dict.fromkeys(sentences))  # preserve order while deduplicating\n",
    "print(f\"Collected {len(sentences):,} sentences after filtering + deduplication.\")\n",
    "\n",
    "for out in OUTPUT_PATHS:\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out.open(\"wb\") as f:\n",
    "        pickle.dump(sentences, f)\n",
    "    print(f\"Saved sentences to {out}\")\n",
    "\n",
    "ROOT / \"data\" / \"librispeech_sentences.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493b9fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x217345d9dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters / config collected here for quick tuning\n",
    "DATA_DIR = Path(\"data\")\n",
    "CMU_PATH = DATA_DIR / \"cmudict-0.7b\"\n",
    "SENTENCE_PKL = DATA_DIR / \"librispeech_sentences.pkl\"\n",
    "\n",
    "MAX_SENTENCES = 6_000_000\n",
    "MAX_LEN = 128\n",
    "TRAIN_SAMPLES = 4_000_000\n",
    "VAL_SAMPLES = 10_000\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 10\n",
    "SEED = 0\n",
    "\n",
    "NOISE = (0.1, 0.1, 0.1)  # (p_sub, p_ins, p_del)\n",
    "D_MODEL = 256\n",
    "N_HEAD = 8\n",
    "ATTENTION_MODE = \"bidir\"  # options: 'causal', 'limited', 'bidir'\n",
    "LOOKAHEAD = 1  # used when ATTENTION_MODE == 'limited'\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "DIM_FEEDFORWARD = 1024\n",
    "DROPOUT = 0.2\n",
    "\n",
    "BASE_LR = 1e-3\n",
    "END_LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_PCT = 0.1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "MIXED_PRECISION = True\n",
    "USE_TF32 = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rng = random.Random(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04643be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134373, 6078025)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CMUdict and LibriSpeech sentences\n",
    "if not CMU_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing CMUdict at {CMU_PATH}\")\n",
    "if not SENTENCE_PKL.exists():\n",
    "    raise FileNotFoundError(f\"Missing LibriSpeech sentences at {SENTENCE_PKL}\")\n",
    "\n",
    "stress_digits = \"0123456789\"\n",
    "cmu_map: Dict[str, List[List[str]]] = {}\n",
    "with CMU_PATH.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\";;;\"):\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        word, phones = parts[0], parts[1:]\n",
    "        clean = [ph.translate({ord(d): None for d in stress_digits}) for ph in phones]\n",
    "        cmu_map.setdefault(word.lower(), []).append(clean)\n",
    "\n",
    "with SENTENCE_PKL.open(\"rb\") as f:\n",
    "    raw_sentences: List[str] = pickle.load(f)\n",
    "\n",
    "len(cmu_map), len(raw_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1476b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme vocab (40): AA, AE, AH, AO, AW, AY, B, CH, D, DH, EH, ER, EY, F, G, HH, IH, IY, JH, K, L, M, N, NG, OW, OY, P, R, S, SH, T, TH, UH, UW, V, W, Y, Z, ZH, <sil>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43, [('AA', 3), ('AE', 4), ('AH', 5), ('AO', 6), ('AW', 7)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build phoneme vocabulary (pad=0, sos=1, eos=2)\n",
    "ALL_PHONEMES = sorted(\n",
    "    set(ph for word_prons in cmu_map.values() for phoneme_seq in word_prons for ph in phoneme_seq)\n",
    ")\n",
    "if \"<sil>\" not in ALL_PHONEMES:\n",
    "    ALL_PHONEMES.append(\"<sil>\")\n",
    "else:\n",
    "    ALL_PHONEMES = [ph for ph in ALL_PHONEMES if ph != \"<sil>\"] + [\"<sil>\"]\n",
    "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2\n",
    "phoneme_to_idx: Dict[str, int] = {ph: i + 3 for i, ph in enumerate(ALL_PHONEMES)}\n",
    "phoneme_to_idx[\"<pad>\"] = PAD_IDX\n",
    "phoneme_to_idx[\"<sos>\"] = SOS_IDX\n",
    "phoneme_to_idx[\"<eos>\"] = EOS_IDX\n",
    "idx_to_phoneme = {i: p for p, i in phoneme_to_idx.items()}\n",
    "\n",
    "print(f\"Phoneme vocab ({len(ALL_PHONEMES)}): {', '.join(ALL_PHONEMES)}\")\n",
    "\n",
    "vocab_size = len(phoneme_to_idx)\n",
    "\n",
    "CONFUSABLE_GROUPS = [\n",
    "    [\"DH\", \"TH\"],\n",
    "    [\"IH\", \"IY\"],\n",
    "    [\"AH\", \"AA\", \"AE\"],\n",
    "    [\"EH\", \"AE\"],\n",
    "    [\"S\", \"Z\"],\n",
    "    [\"SH\", \"ZH\"],\n",
    "    [\"P\", \"B\"],\n",
    "    [\"T\", \"D\"],\n",
    "    [\"K\", \"G\"],\n",
    "    [\"F\", \"V\"],\n",
    "    [\"CH\", \"JH\"],\n",
    "]\n",
    "neighbor_map: Dict[int, List[int]] = {}\n",
    "for group in CONFUSABLE_GROUPS:\n",
    "    ids = [phoneme_to_idx[p] for p in group if p in phoneme_to_idx]\n",
    "    for pid in ids:\n",
    "        neighbor_map[pid] = [q for q in ids if q != pid]\n",
    "\n",
    "vocab_size, list(phoneme_to_idx.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a12379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 4,315,739 sentence records (from 6,000,000 samples).\n"
     ]
    }
   ],
   "source": [
    "WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "def sentence_to_phonemes(text: str) -> Optional[List[str]]:\n",
    "    words = WORD_RE.findall(text.lower())\n",
    "    if not words:\n",
    "        return None\n",
    "    phonemes: List[str] = []\n",
    "    for i, w in enumerate(words):\n",
    "        prons = cmu_map.get(w)\n",
    "        if not prons:\n",
    "            return None\n",
    "        phones = rng.choice(prons)\n",
    "        phonemes.extend(phones)\n",
    "        if i < len(words) - 1:\n",
    "            phonemes.append(\"<sil>\")\n",
    "    return phonemes\n",
    "\n",
    "if MAX_SENTENCES and len(raw_sentences) > MAX_SENTENCES:\n",
    "    sampled_sentences = rng.sample(raw_sentences, MAX_SENTENCES)\n",
    "else:\n",
    "    sampled_sentences = list(raw_sentences)\n",
    "\n",
    "sentence_records: List[Dict[str, object]] = []\n",
    "for sent in sampled_sentences:\n",
    "    phonemes = sentence_to_phonemes(sent)\n",
    "    if phonemes:\n",
    "        sentence_records.append({\"text\": sent, \"phonemes\": phonemes})\n",
    "\n",
    "if not sentence_records:\n",
    "    raise RuntimeError(\"No sentences could be converted with CMUdict coverage.\")\n",
    "print(f\"Prepared {len(sentence_records):,} sentence records (from {len(sampled_sentences):,} samples).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df07a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy:  ['AA', 'N', 'D', 'NG', 'R', 'EY', '<sil>', 'D', 'EH', 'Z', '<sil>', 'AA', 'R', 'T', 'ZH', 'N', 'AO']\n",
      "clean:  ['AA', 'N', 'D', 'R', 'EY', '<sil>', 'D', 'EH', 'S', '<sil>', 'AA', 'R', 'T', 'S', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "def add_noise(\n",
    "    seq: List[int],\n",
    "    vocab_size: int,\n",
    "    p_sub: float = 0.1,\n",
    "    p_ins: float = 0.05,\n",
    "    p_del: float = 0.05,\n",
    ") -> List[int]:\n",
    "    noisy: List[int] = []\n",
    "    for token in seq:\n",
    "        if random.random() < p_del:\n",
    "            continue\n",
    "        if random.random() < p_sub:\n",
    "            neighbors = neighbor_map.get(token, [])\n",
    "            if neighbors and random.random() < 0.8:\n",
    "                token = random.choice(neighbors)\n",
    "            else:\n",
    "                token = random.randint(3, vocab_size - 1)\n",
    "        noisy.append(token)\n",
    "        if random.random() < p_ins:\n",
    "            noisy.append(random.randint(3, vocab_size - 1))\n",
    "    return noisy\n",
    "\n",
    "\n",
    "def encode_phonemes(phonemes: List[str], max_len: int) -> List[int]:\n",
    "    tokens = [phoneme_to_idx[p] for p in phonemes if p in phoneme_to_idx]\n",
    "    tokens = tokens[: max_len - 2]\n",
    "    tokens.append(EOS_IDX)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def pad_sequence(seq: List[int], max_len: int, pad_value: int = PAD_IDX) -> List[int]:\n",
    "    return seq + [pad_value] * (max_len - len(seq))\n",
    "\n",
    "\n",
    "class PhonemeCorrectionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        records: List[Dict[str, object]],\n",
    "        max_len: int = MAX_LEN,\n",
    "        num_samples: Optional[int] = None,\n",
    "        noise: Tuple[float, float, float] = NOISE,\n",
    "    ) -> None:\n",
    "        self.records = records\n",
    "        self.max_len = max_len\n",
    "        self.num_samples = num_samples or len(records)\n",
    "        self.noise = noise\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        clean_ph: List[str] = self.records[idx % len(self.records)][\"phonemes\"]  # type: ignore[index]\n",
    "        tgt_tokens = encode_phonemes(clean_ph, self.max_len)\n",
    "        noisy_tokens = add_noise(\n",
    "            tgt_tokens,\n",
    "            vocab_size,\n",
    "            p_sub=self.noise[0],\n",
    "            p_ins=self.noise[1],\n",
    "            p_del=self.noise[2],\n",
    "        )\n",
    "        noisy_tokens = noisy_tokens[: self.max_len]\n",
    "        tgt_tokens = tgt_tokens[: self.max_len]\n",
    "        return torch.tensor(noisy_tokens), torch.tensor(tgt_tokens)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    srcs, tgts = zip(*batch)\n",
    "    max_src = min(max(len(s) for s in srcs), MAX_LEN)\n",
    "    max_tgt = min(max(len(t) for t in tgts) + 2, MAX_LEN)  # room for <sos>/<eos>\n",
    "\n",
    "    padded_src = [pad_sequence(s.tolist()[:max_src], max_src) for s in srcs]\n",
    "    tgt_in, tgt_out = [], []\n",
    "    for tgt in tgts:\n",
    "        tokens = tgt.tolist()[: max_tgt - 2]\n",
    "        seq = [SOS_IDX] + tokens + [EOS_IDX]\n",
    "        seq = seq[:max_tgt]\n",
    "        seq += [PAD_IDX] * (max_tgt - len(seq))\n",
    "        tgt_in.append(seq[:-1])\n",
    "        tgt_out.append(seq[1:])\n",
    "\n",
    "    return torch.tensor(padded_src), torch.tensor(tgt_in), torch.tensor(tgt_out)\n",
    "\n",
    "\n",
    "# quick sanity check\n",
    "_ds = PhonemeCorrectionDataset(sentence_records, max_len=32, num_samples=5)\n",
    "for noisy, clean in _ds:\n",
    "    print(\"noisy: \", [idx_to_phoneme[i] for i in noisy.tolist() if i != PAD_IDX])\n",
    "    print(\"clean: \", [idx_to_phoneme[i] for i in clean.tolist() if i != PAD_IDX])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5232f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, : x.size(1)].to(x.device)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
    "    mask = torch.full((sz, sz), float('-inf'), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    mask.fill_diagonal_(0.0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_limited_future_mask(sz: int, device: torch.device, lookahead: int = 1) -> torch.Tensor:\n",
    "    # Allow attention to current position and up to `lookahead` future steps; disallow further lookahead.\n",
    "    i = torch.arange(sz, device=device).unsqueeze(1)\n",
    "    j = torch.arange(sz, device=device)\n",
    "    return torch.where(j - i > lookahead, float('-inf'), 0.0)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = D_MODEL,\n",
    "        nhead: int = N_HEAD,\n",
    "        num_encoder_layers: int = NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers: int = NUM_DECODER_LAYERS,\n",
    "        dim_feedforward: int = DIM_FEEDFORWARD,\n",
    "        dropout: float = DROPOUT,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.tgt_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def encode(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        src_key_padding_mask = src == PAD_IDX\n",
    "        if ATTENTION_MODE == 'bidir':\n",
    "            src_mask = None\n",
    "        elif ATTENTION_MODE == 'limited':\n",
    "            src_mask = generate_limited_future_mask(src.size(1), src.device, lookahead=LOOKAHEAD)\n",
    "        else:\n",
    "            src_mask = generate_square_subsequent_mask(src.size(1), src.device)\n",
    "        return self.encoder(\n",
    "            self.pos_enc(self.src_emb(src)),\n",
    "            mask=src_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor) -> torch.Tensor:\n",
    "        src_key_padding_mask = src == PAD_IDX\n",
    "        tgt_key_padding_mask = tgt_in == PAD_IDX\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_in.size(1), tgt_in.device) if ATTENTION_MODE == 'causal' else None\n",
    "\n",
    "        src_h = self.encode(src)\n",
    "        tgt_h = self.decoder(\n",
    "            self.pos_enc(self.tgt_emb(tgt_in)),\n",
    "            src_h,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "        return self.proj(tgt_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7211a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_21520\\1448833290.py:58: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=MIXED_PRECISION and DEVICE.type == \"cuda\")\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "train_samples = min(TRAIN_SAMPLES, len(sentence_records))\n",
    "val_samples = min(VAL_SAMPLES, max(0, len(sentence_records) - train_samples))\n",
    "\n",
    "# explicit train/val split with deterministic shuffle\n",
    "split_records = list(sentence_records)\n",
    "rng.shuffle(split_records)\n",
    "train_records = split_records[:train_samples]\n",
    "val_records = split_records[train_samples: train_samples + val_samples] or split_records[-val_samples:]\n",
    "\n",
    "train_ds = PhonemeCorrectionDataset(train_records, max_len=MAX_LEN, num_samples=len(train_records), noise=NOISE)\n",
    "val_ds = PhonemeCorrectionDataset(val_records, max_len=MAX_LEN, num_samples=len(val_records), noise=NOISE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "if DEVICE.type == \"cuda\" and USE_TF32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "total_train_iters = max(1, NUM_EPOCHS * len(train_loader))\n",
    "warmup_iters = max(1, int(WARMUP_PCT * total_train_iters))\n",
    "min_lr_factor = END_LR / BASE_LR\n",
    "\n",
    "\n",
    "def lr_lambda(step: int) -> float:\n",
    "    if step < warmup_iters:\n",
    "        return (step + 1) / warmup_iters\n",
    "    remaining = max(total_train_iters - warmup_iters, 1)\n",
    "    decay_step = step - warmup_iters\n",
    "    decay_frac = max(0.0, 1.0 - decay_step / remaining)\n",
    "    return max(min_lr_factor, decay_frac)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scaler = GradScaler(enabled=MIXED_PRECISION and DEVICE.type == \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4470b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_phonemes(tokens: List[int]) -> List[str]:\n",
    "    return [idx_to_phoneme.get(int(t), f\"<{t}>\") for t in tokens if int(t) not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "\n",
    "\n",
    "def greedy_decode(src: torch.Tensor, max_len: int = MAX_LEN) -> List[int]:\n",
    "    model.eval()\n",
    "    src = src.unsqueeze(0).to(DEVICE)\n",
    "    src_key_padding_mask = src == PAD_IDX\n",
    "    memory = model.encode(src)\n",
    "    ys = torch.tensor([[SOS_IDX]], device=DEVICE)\n",
    "    for _ in range(max_len):\n",
    "        tgt_mask = generate_square_subsequent_mask(ys.size(1), ys.device) if ATTENTION_MODE == 'causal' else (generate_limited_future_mask(ys.size(1), ys.device, lookahead=LOOKAHEAD) if ATTENTION_MODE == 'limited' else None)\n",
    "        out = model.decoder(\n",
    "            model.pos_enc(model.tgt_emb(ys)),\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=None,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "        logits = model.proj(out[:, -1, :])\n",
    "        next_word = logits.argmax(dim=-1).unsqueeze(0)\n",
    "        ys = torch.cat([ys, next_word], dim=1)\n",
    "        if next_word.item() == EOS_IDX:\n",
    "            break\n",
    "    return ys.squeeze(0).tolist()\n",
    "\n",
    "\n",
    "def log_val_example(example_idx: int = 0) -> None:\n",
    "    model.eval()\n",
    "    noisy, tgt = val_ds[example_idx % len(val_ds)]\n",
    "    rec = val_ds.records[example_idx % len(val_ds)]\n",
    "    text = rec.get('text', '') if isinstance(rec, dict) else ''\n",
    "    pred_tokens = greedy_decode(noisy, max_len=MAX_LEN)\n",
    "\n",
    "    noisy_ph = tokens_to_phonemes(noisy.tolist())\n",
    "    tgt_ph = tokens_to_phonemes(tgt.tolist())\n",
    "    pred_ph = tokens_to_phonemes(pred_tokens)\n",
    "\n",
    "    print(f\"[val example] noisy:        {' '.join(noisy_ph)}\")\n",
    "    print(f\"[val example] target:       {' '.join(tgt_ph)}\")\n",
    "    print(f\"[val example] corrected:    {' '.join(pred_ph) if pred_ph else '<empty>'}\")\n",
    "    if text:\n",
    "        print(f\"[val example] plain text:   {text}\")\n",
    "\n",
    "\n",
    "def run_epoch(loader, train: bool = True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    accum = GRAD_ACCUM_STEPS if train else 1\n",
    "    if train:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for step, (src, tgt_in, tgt_out) in enumerate(loader):\n",
    "        src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
    "\n",
    "        # autocast may not accept a device_type kwarg on all torch versions;\n",
    "        # use enabled=scaler.is_enabled() which already reflects MIXED_PRECISION & CUDA\n",
    "        with autocast(enabled=scaler.is_enabled()):\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), tgt_out.reshape(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if train:\n",
    "            loss = loss / accum\n",
    "            scaler.scale(loss).backward()\n",
    "            do_step = (step + 1) % accum == 0 or (step + 1) == num_batches\n",
    "            if do_step:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scheduler.step()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10de09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint 10\n",
    "ckpt_path = Path(\"checkpoints\") / \"phoneme_seq2seq_epoch_10.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "BATCH_SIZE = 1024\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e702f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnn\\AppData\\Local\\Temp\\ipykernel_21520\\3821134528.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=scaler.is_enabled()):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m NUM_EPOCHS):\n\u001b[1;32m----> 2\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 71\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_step:\n\u001b[0;32m     70\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     73\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:462\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m )\n\u001b[1;32m--> 462\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    464\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:356\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    355\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    357\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\johnn\\Desktop\\EC ENGR C143A\\c143a-project\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:356\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    355\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    357\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10, 10 + NUM_EPOCHS):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    print(f\"epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | lr={scheduler.get_last_lr()[0]:.6f}\")\n",
    "    log_val_example(example_idx=epoch)\n",
    "\n",
    "    # checkpoint every epoch\n",
    "    ckpt_path = Path(\"checkpoints\") / f\"phoneme_seq2seq_epoch_{epoch+1}.pt\"\n",
    "    ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"phoneme_to_idx\": phoneme_to_idx,\n",
    "            \"idx_to_phoneme\": idx_to_phoneme,\n",
    "            \"pad_idx\": PAD_IDX,\n",
    "            \"sos_idx\": SOS_IDX,\n",
    "            \"eos_idx\": EOS_IDX,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"model_kwargs\": {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"d_model\": D_MODEL,\n",
    "                \"nhead\": N_HEAD,\n",
    "                \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
    "                \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
    "                \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "                \"dropout\": DROPOUT,\n",
    "            },\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "    print(f\"Saved epoch {epoch+1} checkpoint to {ckpt_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbe54d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy    : DH EH Y <sil> HH IY <sil> S T R AY Z W IH DH <sil> HH IH Z <sil> SH R AA R P EH JH D <sil> F R TH AH Z N <sil> HH UH F S <sil> AE D <sil> DH NG EY <sil> AA AY <sil> T EH AH UW AH L F <sil> W EH AY P AH N Z <eos>\n",
      "Target   : DH EH N <sil> HH IY <sil> S T R AY K S <sil> W IH DH <sil> HH IH Z <sil> SH AA R P <sil> EH JH D <sil> F R AH N T <sil> HH UH F S <sil> AH N D <sil> DH EY <sil> AA R <sil> T EH R AH B AH L <sil> W EH P AH N Z\n",
      "Pred     : DH EH N <sil> HH IY <sil> S T R AY Z <sil> W IH DH <sil> HH IH Z <sil> SH AA R P <sil> EH JH D <sil> F R AH N <sil> HH UH F S <sil> AE T <sil> DH EY <sil> AA R <sil> T UW <sil> AH V <sil> W EH P AH L <sil> W AH N Z\n",
      "Noisy    : EH IH G IY V <sil> EH EH R IY AH V <sil> R IY Z AH N P <sil> F AO R <sil> D L EY N <sil> ER JH Z IY NG <sil> R IH P IY T IH D L IY <sil> G DH AE T <sil> DH <sil> AH <sil> K W CH EH S CH AH N <sil> AH <sil> HH Z IH <sil> M EH R IH R <sil> W AA Z W AH N SH <sil> IH CH HH IY ER K UH D UH <sil> AA T <sil> EH AO Z <sil> AH P AA N <sil> AH <sil> T UW K <sil> Z OW <sil> S UW <sil> F T ER <sil> DH <eos>\n",
      "Target   : HH IY <sil> G EY V <sil> V EH R IY AH S <sil> R IY Z AH N Z <sil> F AO R <sil> D IH L EY <sil> ER JH IH NG <sil> R IH P IY T IH D L IY <sil> DH AE T <sil> DH AH <sil> K W EH S CH AH N <sil> AH V <sil> HH IH Z <sil> M EH R IH JH <sil> W AA Z <sil> W AH N <sil> W IH CH <sil> HH IY <sil> K UH D <sil> N AA T <sil> P R EH S <sil> AH P AA N <sil> DH AH <sil> D UW K <sil> S OW <sil> S UW N <sil> AE F T ER <sil> DH\n",
      "Pred     : EH V ER IY <sil> V EH R IY <sil> AH V <sil> R IY Z AH N <sil> F AO R <sil> D EY N <sil> ER JH IH NG <sil> R IH P IY <sil> D IH D L IY <sil> DH AE T <sil> DH AH <sil> K W EH S CH AH N <sil> HH IH M <sil> DH AH <sil> HH IH R <sil> W AH N <sil> HH IH R <sil> W AA Z <sil> W AH N <sil> W IH CH <sil> HH IY <sil> K UH D <sil> N AA T <sil> AE Z <sil> AH P AA N <sil> AH <sil> T UW <sil> S OW <sil> S UW T <sil> AE F T ER <sil> DH\n",
      "Noisy    : AH D <sil> HH L IY P Y ER HH P Z <sil> N UW <sil> N AA T <sil> S OW AE W OY EH L <sil> HH AW UW T <sil> M EY K DH AH <sil> JH M IH K S Y CH ER Z <sil> N AW OW DH AE D <sil> HH AW IY <sil> N OW Z <sil> L EH D <sil> G HH IH M UW Z <sil> DH IH T <sil> B Z AY <sil> HH IH M EH L F <sil> S IH JH <sil> AY <sil> EH M <sil> N AO OW <sil> M F UH P D <sil> HH EY IY R <eos>\n",
      "Target   : B AH T <sil> HH IY <sil> P ER HH AE P S <sil> N UW <sil> N AA T <sil> S OW <sil> W EH L <sil> HH AW <sil> T UW <sil> M EY K <sil> DH AH <sil> M IH K S CH ER Z <sil> N AW <sil> DH AE T <sil> HH IY <sil> N OW Z <sil> L EH T <sil> HH IH M <sil> D UW <sil> IH T <sil> B AY <sil> HH IH M S EH L F <sil> S IH N S <sil> AY <sil> AE M <sil> N OW <sil> M AO R <sil> G UH D <sil> HH IY R\n",
      "Pred     : AH N D <sil> HH IY <sil> P ER HH AE P S <sil> N UW <sil> N AA T <sil> S OW <sil> W EH L <sil> HH AW <sil> T UW <sil> M EY K <sil> DH AH <sil> M IH K S CH ER Z <sil> N AW <sil> DH AE T <sil> HH IY <sil> N OW Z <sil> L EH D <sil> HH IH M <sil> DH IH S <sil> B AY <sil> HH IH M <sil> S EH L <sil> AY <sil> AE M <sil> N OW <sil> M AY <sil> P EY L <sil> HH IY R\n"
     ]
    }
   ],
   "source": [
    "# Decode a few examples (greedy autoregressive)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    src_batch, tgt_batch, _ = next(iter(val_loader))\n",
    "    src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
    "\n",
    "for i in range(3):\n",
    "    noisy_tokens = [idx_to_phoneme[t.item()] for t in src_batch[i] if t.item() != PAD_IDX]\n",
    "    clean_tokens = [idx_to_phoneme[t.item()] for t in tgt_batch[i] if t.item() not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "    pred_tokens = greedy_decode(src_batch[i].cpu(), max_len=MAX_LEN)\n",
    "    pred = [idx_to_phoneme.get(int(t), f\"<{t}>\") for t in pred_tokens if int(t) not in (PAD_IDX, SOS_IDX, EOS_IDX)]\n",
    "    print(f\"Noisy    : {' '.join(noisy_tokens)}\")\n",
    "    print(f\"Target   : {' '.join(clean_tokens)}\")\n",
    "    print(f\"Pred     : {' '.join(pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50833655",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(\"/phoneme_seq2seq.pt\")\n",
    "export_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"phoneme_to_idx\": phoneme_to_idx,\n",
    "    \"idx_to_phoneme\": idx_to_phoneme,\n",
    "    \"pad_idx\": PAD_IDX,\n",
    "    \"sos_idx\": SOS_IDX,\n",
    "    \"eos_idx\": EOS_IDX,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"model_kwargs\": {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"d_model\": D_MODEL,\n",
    "        \"nhead\": N_HEAD,\n",
    "        \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
    "        \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
    "        \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "        \"dropout\": DROPOUT,\n",
    "    },\n",
    "}\n",
    "torch.save(checkpoint, export_path)\n",
    "print(f\"Saved phoneme seq2seq checkpoint to {export_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c143a-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
