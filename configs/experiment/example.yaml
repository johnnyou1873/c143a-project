# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: speech
  - override /model: speech_diphone  # uses SpeechModuleCTCWithPS2SLM
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["speech", "ps2s"]

# seed: 0

trainer:
  min_epochs: 1 # prevents early stopping
  max_epochs: 50
  # limit_val_batches: 10
  # limit_test_batches: 10
  # num_sanity_val_steps: 0
  # gradient_clip_val: 0.5

model:
  # override top-level speech model hyperparameters
  lr_start: 1e-5
  lr_end: 1e-6
  n_units: 1024
  # n_layers: 8
  # nhead: 6
  # dropout: 0.2
  gaussian_smooth_width: 2.0
  # stride_len: 2         # temporal subsampling stride
  # kernel_len: 15        # temporal subsampling kernel
  # l2_decay: 1e-5      # inherited from model config
  # seed: ${seed}
  # compile: False # disable compile for debugging
  # Explicitly set pretrained checkpoints for GRU and PS2S
  gru_ckpt_path: data/gru1024-diphone.ckpt
  ps2s_model_path: data/phoneme_seq2seq.pt

data:
  batch_size: 32
  white_noise_std: 0.8
  constant_offset_std: 0.2
  num_workers: 4

logger:
  wandb:
    name: "GRU1024 Diphone with PS2S"
    tags: ${tags}
    group: "speech"
  aim:
    experiment: "speech"
