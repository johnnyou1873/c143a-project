# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: speech
  - override /model: LLMSpeech
  - override /callbacks: default
  - override /trainer: default
  - override /logger: csv

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["speech", "gru_decoder"]

# seed: 0

trainer:
  min_epochs: 1 # prevents early stopping
  max_epochs: 100
  # gradient_clip_val: 0.5

model:
  # override top-level speech model hyperparameters
  # lr_start: 0.001
  # lr_end: 0.0001
  # n_units: 1024
  # n_layers: 5
  # dropout: 0.4
  # gaussian_smooth_width: 2.0
  # stride_len: 4
  # kernel_len: 32
  # bidirectional: False
  # l2_decay: 0.1
  # seed: ${seed}
  # compile: False # disable compile for debugging
  # LLM guidance
  llm_enabled: True
  llm_weight: 0.8
  llm_model_name: gemma3:4b
  llm_max_new_tokens: 150
  llm_temperature: 0.5
  llm_prompt: >
    You are assisting with English phoneme recognition. Given a rough phoneme prediction,
    return a cleaned list of phonemes using only the provided set.

# datamodule overrides
data: {}
# data:
#   batch_size: 16
#   seq_len: 150
#   max_time_series_len: 1200
#   white_noise_std: 0.8
#   constant_offset_std: 0.2
#   n_classes: 40
#   n_input_features: 256
#   seed: ${seed}
#   num_workers: 8
#   persistent_workers: True

logger:
  wandb:
    tags: ${tags}
    group: "speech"
  aim:
    experiment: "speech"
