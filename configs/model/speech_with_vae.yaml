_target_: src.models.speech_module_with_vae.SpeechModuleVAE

# general hyperparameters
lr_start: 0.000075
lr_end: 0.00000075
n_units: 1024
n_batch: 10000
n_layers: 5
n_classes: 40
n_input_features: 256
dropout: 0.4
gaussian_smooth_width: 2.0
stride_len: 4
kernel_len: 32
bidirectional: False
l2_decay: 0.1
seed: 0

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: ${..lr_start}
  weight_decay: ${..l2_decay}

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  _partial_: true
  T_max: ${trainer.max_epochs}
  eta_min: ${..lr_end}

# warmup scheduler is not helpful here
# scheduler:
#   _target_: transformers.get_cosine_schedule_with_warmup
#   _partial_: true
#   optimizer: ${..optimizer}
#   num_warmup_steps: 0
#   num_training_steps: ${trainer.max_epochs}

net:
  _target_: src.models.components.gru_decoder_with_vae.GRUDecoderVAE
  neural_dim: ${..n_input_features}
  hidden_dim: ${..n_units}
  hidden_dim_vae: 512
  latent_global: 18
  latent_local: 110
  layer_dim: ${..n_layers}
  n_classes: ${..n_classes}
  nDays: 24
  dropout: ${..dropout}
  strideLen: ${..stride_len}
  kernelLen: ${..kernel_len}
  gaussianSmoothWidth: ${..gaussian_smooth_width}
  bidirectional: ${..bidirectional}

# compile model for faster training with pytorch 2.0
compile: false
